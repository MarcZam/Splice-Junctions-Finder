}
nb_train_labels <- factor(modifyLabels(df_train_labels), levels =c("SJ", "N"))
nb_test_labels <- factor(modifyLabels(df_test_labels), levels =c("SJ", "N"))
install.packages("randomForest")
install.packages("naivebayes")
SVM_class <- function(df_train_data, df_test, df_test_labels, kernel = c("rbfdot", "vanilladot")){
for(i in kernel){
SVMmodel <- ksvm(class ~ ., data = train_data, kernel = i)
SVM_pred <- predict(SVMmodel, df_test)
tab <- table(RBFMod_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
SVM_class(df_train_data, df_test, df_test_labels)
SVM_class <- function(df_train_data, df_test, df_test_labels, kernel = c("rbfdot", "vanilladot")){
for(i in kernel){
SVMmodel <- ksvm(class ~ ., data = train_data, kernel = i)
SVM_pred <- predict(SVMmodel, df_test)
tab <- table(RBFMod_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
SVM_class(df_train_data, df_test, df_test_labels)
install.packages("neuralnet")
install.packages("OneR")
install.packages("c50")
install.packages("C50")
library(C50)
Tree_class <- function(df_train, df_test, df_train_labels, df_test_labels, boosting = c(0, 1)){
for(i in boosting){
TreeModel <- C5.0(df_train, df_train_labels, kernel = i)
TM_pred <- predict(TreeModel, df_test)
tab <- table(TM_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Tree_class(df_train, df_test, df_train_labels, df_test_labels)
SVM_class <- function(df_train_data, df_test, df_test_labels, kernel = c("rbfdot", "vanilladot")){
for(i in kernel){
SVMmodel <- ksvm(class ~ ., data = df_train_data, trials = i)
SVM_pred <- predict(SVMmodel, df_test)
tab <- table(SVM_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
SVM_class(df_train_data, df_test, df_test_labels, kernel = "rbfdot")
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
setwd('D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.163 - Machine Learning/Splice-Junctions-Finder')
library(dplyr)
library(kableExtra)
library(caret)
library(class)
library(gmodels)
library(e1071)
library(kernlab)
library(neuralnet)
library(c50)
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
setwd('D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.163 - Machine Learning/Splice-Junctions-Finder')
library(dplyr)
library(kableExtra)
library(caret)
library(class)
library(gmodels)
library(e1071)
library(kernlab)
library(neuralnet)
library(C50)
SVM_class <- function(df_train_data, df_test, df_test_labels, kernel = c("rbfdot", "vanilladot")){
for(i in kernel){
SVMmodel <- ksvm(class ~ ., data = df_train_data, trials = i)
SVM_pred <- predict(SVMmodel, df_test)
tab <- table(SVM_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
SVM_class(df_train_data, df_test, df_test_labels, kernel = "rbfdot")
tt_split <- function(df, train_percentage){
set.seed(123)
# Aleatoreizamos las filas del dataset introducido para que si están ordenadas, esto no afecte a las predicciones del modelo
rows <- sample(nrow(df), replace =FALSE)
df <- df[rows, ]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
percent <- round(((nrow(SpliceData)/100) * train_percentage), 0)
df_train_data <<- df[1:percent, -2]
df_train <<- df[1:percent, -c(1,2)]
df_test <<- df[(percent+1):nrow(df), -c(1,2)]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
df_train_labels <<- c(df[1:percent, 1])
df_test_labels <<- c(df[(percent+1):nrow(df),1])
}
tt_split(SpliceData, 67)
SVM_class <- function(df_train_data, df_test, df_test_labels, kernel = c("rbfdot", "vanilladot")){
for(i in kernel){
SVMmodel <- ksvm(class ~ ., data = df_train_data, trials = i)
SVM_pred <- predict(SVMmodel, df_test)
tab <- table(SVM_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
SVM_class(df_train_data, df_test, df_test_labels, kernel = "rbfdot")
install.packages("ranger", "ordinalForest")
RandomF_class <- function(df_train_data, df_test, df_test_labels, n = c(50, 100)){
for(i in n){
RandomFModel <- train(class ~ ., data = df_train_data, method = "ordinalRF", ntreefinal = i)
RandomFModel_pred <- predict(RandomFModel, df_test)
tab <- table(RandomFModel_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Tree_class(df_train_data, df_test, df_test_labels)
Tree_class <- function(df_train, df_test, df_train_labels, df_test_labels, boosting = c(0, 1)){
for(i in boosting){
TreeModel <- C5.0(df_train, df_train_labels, kernel = i)
TM_pred <- predict(TreeModel, df_test)
tab <- table(TM_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Tree_class(df_train, df_test, df_train_labels, df_test_labels)
RandomF_class <- function(df_train_data, df_test, df_test_labels, n = c(50, 100)){
for(i in n){
RandomFModel <- train(class ~ ., data = df_train_data, method = "ordinalRF", ntreefinal = i)
RandomFModel_pred <- predict(RandomFModel, df_test)
tab <- table(RandomFModel_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Tree_class(df_train_data, df_test, df_test_labels)
install.packages("randomforest")
install.packages("randomForest")
RandomF_class <- function(df_train, df_test, df_train_labels, df_test_labels, n = c(50, 100)){
for(i in n){
RandomFModel <- randomForest(df_train, df_train_labels, maxnodes = i)
RandomFModel_pred <- predict(RandomFModel, df_test)
tab <- table(RandomFModel_pred, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Tree_class(df_train, df_test, df_train_labels, df_test_labels)
install.packages("h2o")
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
setwd('D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.163 - Machine Learning/Splice-Junctions-Finder')
library(dplyr)
library(kableExtra)
library(caret)
library(class)
library(gmodels)
library(e1071)
library(kernlab)
library(neuralnet)
library(C50)
library(randomForest)
library(h2o)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init(max_mem_size = "5g")  # iinicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = seq_along(features),
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
sparse = TRUE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = seq_along(features),
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
sparse = TRUE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(ae1_codings, df_test, df_train_labels, df_test_labels, k = 7)
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(ae1_codings, df_test, df_train_labels, df_test_labels, k = 7)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = seq_along(features),
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = seq_along(features),
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = F
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(ae1_codings, df_test, df_train_labels, df_test_labels, k = 7)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = seq_along(features),
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = df_train,
y = df_train_labels
training_frame = df_train_data,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = df_train,
y = df_train_labels,
training_frame = df_train_data,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train_data)
ae1 <- h2o.deeplearning(
x = df_train,
y = df_train_labels,
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
tt_split <- function(df, train_percentage){
set.seed(123)
# Aleatoreizamos las filas del dataset introducido para que si están ordenadas, esto no afecte a las predicciones del modelo
rows <- sample(nrow(df), replace =FALSE)
df <- df[rows, ]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
percent <- round(((nrow(SpliceData)/100) * train_percentage), 0)
df_train_data <<- df[1:percent, -2]
df_train <<- df[1:percent, -c(1,2)]
df_test <<- df[(percent+1):nrow(df), -c(1,2)]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
df_train_labels <<- c(df[1:percent, 1])
df_test_labels <<- c(df[(percent+1):nrow(df),1])
}
tt_split(SpliceData, 67)
ncol(df_train_data)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train_data)
ae1 <- h2o.deeplearning(
x = features[[2:481]],
y = features[[1]],
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train_data)
ae1 <- h2o.deeplearning(
x = features[[-1]],
y = features[[1]],
training_frame = features,
autoencoder = TRUE,
hidden = 2,
activation = 'Tanh',
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = features,
training_frame = features,
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
features <- as.h2o(df_train)
ae1 <- h2o.deeplearning(
x = features,
training_frame = features,
autoencoder = TRUE,
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(ae1, features, layer = 1)
ae1_codings
View(ae1_codings)
tt_split <- function(df, train_percentage){
set.seed(123)
# Aleatoreizamos las filas del dataset introducido para que si están ordenadas, esto no afecte a las predicciones del modelo
rows <- sample(nrow(df), replace =FALSE)
df <- df[rows, ]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
percent <- round(((nrow(SpliceData)/100) * train_percentage), 0)
df_train_data <<- df[1:percent, -2]
df_train <<- df[1:percent, -c(1,2)]
df_test <<- df[(percent+1):nrow(df), -c(1,2)]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
df_train_labels <<- c(df[1:percent, 1])
df_test_labels <<- c(df[(percent+1):nrow(df),1])
}
tt_split(SpliceData, 67)
ncols(df_train)
tt_split <- function(df, train_percentage){
set.seed(123)
# Aleatoreizamos las filas del dataset introducido para que si están ordenadas, esto no afecte a las predicciones del modelo
rows <- sample(nrow(df), replace =FALSE)
df <- df[rows, ]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
percent <- round(((nrow(SpliceData)/100) * train_percentage), 0)
df_train_data <<- df[1:percent, -2]
df_train <<- df[1:percent, -c(1,2)]
df_test <<- df[(percent+1):nrow(df), -c(1,2)]
# dividimos los data sets en train y test data y extraemos la columna correspondiente a las labels para cada set
df_train_labels <<- c(df[1:percent, 1])
df_test_labels <<- c(df[(percent+1):nrow(df),1])
}
tt_split(SpliceData, 67)
length(df_train)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
1:480,
training_frame = as.h20(df_train),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h20(df_train), layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
1:480,
training_frame = as.h2o(df_train),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
ae1_codings
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(ae1_codings, df_test, df_train_labels, df_test_labels, k = 7)
size(ae1_codings)
ae1_codings=as.matrix(ae1_codings)
size(ae1_codings)
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(ae1_codings, df_test, df_train_labels, df_test_labels, k = 7)
features <- as.matrix(ae1_codings)
features
View(features)
features <- data.frame(ae1_codings)
features
View(df_train_data)
features <- data.frame(ae1_codings)
features
Knn_algor <- function(df_train, df_test, df_train_labels, df_test_labels, k = c(1,3,5,7)){
for(i in k){
model_preds <- knn(train = df_train, test = df_test, cl = df_train_labels, k = i)
tab <- table(model_preds, df_test_labels, dnn = c("Actual", "Predicha"))
print(confusionMatrix(tab))
}
}
Knn_algor(features, df_test, df_train_labels, df_test_labels, k = 7)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
x = 2:480,
y = 1,
training_frame = as.h2o(SpliceData),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
x = 2:480,
training_frame = as.h2o(SpliceData),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
ae1_codings
features <- data.frame(ae1_codings)
features
View(features)
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
x = 2:480,
training_frame = as.h2o(SpliceData),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
x = 2:480,
training_frame = as.h2o(df_train),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
ae1_codings
h2o.no_progress()  # desactivamos las barras de progreso
h2o.init()  # inicializamos una instancia del H2O
model <- h2o.deeplearning(
x = as.h2o(SpliceData),
training_frame = as.h2o(SpliceData),
autoencoder = TRUE,
activation = "Tanh",
ignore_const_cols = FALSE
)
ae1_codings <- h2o.deepfeatures(model, as.h2o(df_train), layer = 1)
features <- as.matrix(ae1_codings)
features
knitr::opts_chunk$set(echo = TRUE, comment = NULL)
setwd('D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.163 - Machine Learning/Splice-Junctions-Finder')
library(dplyr)
library(kableExtra)
library(caret)
library(class)
library(gmodels)
library(e1071)
library(kernlab)
library(neuralnet)
library(C50)
library(randomForest)
library(h2o)
